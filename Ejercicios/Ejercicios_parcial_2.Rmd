 

 ## Packages 
```{r}
library(astsa)
library(tidyverse)
```

# Chapter 1
Exercise 2

```{r}
#a
s <- c(rep(0,100),10*exp(-(1:100)/20)*cos(2*pi*1:100/4))
x <- s+rnorm(200)

#b 
s_b <- c(rep(0,100),10*exp(-(1:100)/200)*cos(2*pi*1:100/4))
x_b <- s_b+rnorm(200)
plot.ts(x) #Explosion
plot.ts(x_b) #Earthquake


plot(exp(-c(0:100)/20),type='l',col='red')

lines(exp(-c(0:100)/200),type='l',col='blue')

```
We find that the line of $\exp{\left(-t/200\right)}$ decrease slower. But given the slower 
decrease it shows less aperature.


Exercise 3

```{r}
w_3 <- rnorm(110,0,1)
x_3 <- filter(w_3,filter  =c(0,-0.9),method = "recursive")[-c(1:10)]
v_3 <- filter(x_3,rep(1/4,4),side = 1)
plot.ts(x_3,main="autoregression",type="l")
lines(v_3,type="l",lty=2,col="blue")
```


```{r}
x_3.b <- cos(2*pi*1:100/4)
x_3.c <- cos(2*pi*1:100/4)+rnorm(100)
v_3.c <- filter(x_3.c,rep(1/4,4),side = 1)
v_3.b<- filter(x_3.b,rep(1/4,4),side = 1)

plot.ts(x_3.b,main = "autoregression",type = "l")
lines(v_3.b,type = "l", lty=2, col="blue")
plot.ts(x_3.c,main = "autoregression",type = "l")
lines(v_3.c,type = "l", lty=2, col="blue")



```


 2.8, 2.10, 3.22, 3.34, 3.35, 3.42, 4.10, 4.15.


Exercise 2.8 

```{r}


data(varve)
varve_split <- split(varve,f = c('A','B')) #We split the data in two
(variance_A <- (sd(varve_split$A))^2)
(variance_B <- (sd(varve_split$B))^2) 
logvarve <- log(varve)
logvarve_split <- split(logvarve,f = c('logA','logB')) 
(variance_logA <- (sd(logvarve_split$logA))^2)
(variance_logB <- (sd(logvarve_split$logB))^2)
```
We notice that after applying the log transformation the variance is closer, it doesn't happen before.

```{r}
#We plot the histograms
hist(varve)
hist(logvarve)

```

After applying the transformation we have a graph that is a little bit symmetric.

```{r}
#We plot the data of logvarve
plot.ts(logvarve)
acf(logvarve)

```

We see I a low drecreasing of the ACF. It is an indication that the serie is not stationary.


```{r}
#First differnce
diff_logvarve <- diff(logvarve)
plot.ts(diff_logvarve)
acf(diff_logvarve)
```

It shows a better behavior as a stationary serie. Because the ACF cutt off
in lag 1.
We can say that it is a rate of change by day of the varve. 

A random walk with a drift 


For justify the part e what we can do 
is estimate the ACF theoric of the model
and watch that the variance is approximately 
the empiric value.

Exercise 2.10
```{r}
data(oil)
data(gas)
ts.plot(oil, gas, col =1:2)# To plot in the same graph

```

Both look more like a random walk with drift.

They are not stationary it looks like there are tendency component, and also it appear that autocovariance
function will depend on the time.

a.
```{r}
acf(oil)
acf(gas)

```

b.
Both decrease slowly. If we apply the log difference it can be instrepeter as the proportion of change or the returns.


c.
```{r}
logoil_diff <- diff(log(oil))
loggas_diff <- diff(log(gas))
ts.plot(logoil_diff,loggas_diff,col = 1:2)
```
We have controlled the variance and the tendency. They look stationary.


```{r}
acf(logoil_diff)
acf(loggas_diff)
```
They still have a little autocorrelation. We can notice it because the autocorrelation in the lags.



d. 
We can watch a high cross relation in the lag 0.
```{r}
ccf(loggas_diff,logoil_diff)
ccf(logoil_diff,loggas_diff)
```

There is a high cross correlation in lag 0.


e.
```{r}
lag2.plot(logoil_diff,loggas_diff,3)


#What lag2.plot does is this
plot(logoil_diff[-length(logoil_diff)],loggas_diff[-1])
lines(lowess(logoil_diff[-length(logoil_diff)],loggas_diff[-1]))

```

The relation looks linear.

f.
```{r}
I <- logoil_diff>=0
merge <- ts.intersect(loggas_diff,logoil_diff,logoil_diff_lag = lag(logoil_diff,-1),I)
fit_oil_gas <- lm(loggas_diff~logoil_diff+logoil_diff_lag+I, data =merge)
summary(fit_oil_gas)
```
All look as to be significant except the intercept. 

When there is an negative growth of the price of oil there is a negative increase in the
gas price in 0.68. There will be steady. 



Analyze of the residuals
```{r}
resid_2_10 <- residuals(fit_oil_gas)
plot(resid_2_10)
acf(resid_2_10)
```

The adjust is not bad, except by the outliers.


Exercise 3.22

```{r}
phi <- 0.9
generating_observations <- arima.sim(n = 50,list(ar = phi),n.start = 50)
yw_fit <- ar.yw(generating_observations,order = 1)
hist(generating_observations)
nboot <- 200
residuals_generated <- yw_fit$resid[-1]
x_star <- generating_observations
phi_star <- rep(NA,nboot)

for(i in 1:nboot){
    resid_star <- sample(residuals_generated, replace = TRUE)
    for(j in 1:49){
        x_star[j+1] <- phi*x_star[j] + resid_star[j]
    }
    phi_star[j] <- ar.yw(x_star,order = 1)$ar
}
summary(phi_star)
```


Exercise 3.34
```{r}
data(so2)
ts.plot(so2)
acf2(so2)
```

We can se that the model is not a stationary. There is a a little of tendency, we are going to the the first difference and 
there is a lot of correlation.

```{r}
diff_so2 <- diff(so2)

ts.plot(diff_so2)
acf(diff_so2)
pacf(diff_so2)

```

After that we control the tendency and there is still a little of correlation. We are going to apply
log transformation and first difference

```{r}
diff_logso2 <- diff(log(so2))

ts.plot(diff_logso2)
acf(diff_logso2)
pacf(diff_logso2)

acf2(diff_logso2)
```
We can see that the difference is not a lot, so we are going only to take first difference without log transformation.

```{r}

ts.plot(diff_so2)
acf(diff_so2)
pacf(diff_so2)
```


We propose that in the acf the tail tail off after lag 1 in both models we have so an ARIMA(1,1,1), and ARIMA(0,1,1).
```{r}
#We fit the model
model_AR_011 <-sarima(so2,0,1,1)
model_AR_011$AIC
model_AR_011$BIC
```

We can see that residulas don't show a a pattern, and also we can say that there is
independece in the residuals. There are a various outliers. With the Ljung Box 
we notice that could be the model doesn't fit well.

```{r}
#We fit the model
model_complete <-sarima(so2,1,1,1)
model_complete$AIC
model_complete$BIC
```
We can see that with complete model there is a little bit of improvement in the Q test, but there is not
plenty improvement. The behavior is similar to the modelo ARIMA(0,1,1), and we see that the 
acf is worse.


```{r}
#We fit the model
modellog_AR_011 <-sarima(log(so2),0,1,1)
modellog_AR_011$AIC
modellog_AR_011$BIC
```
There is an improvement in the outliers, but in the Q test it is a little worse.

```{r}
#We fit the model
modellog_AR_111 <-sarima(log(so2),1,1,1)
modellog_AR_111$AIC
modellog_AR_111$BIC
```


We can select the log transformation and the model that have the AIC, and BIC lower. So,
we select the ARIMA(0,1,1)


Exercise 3.35
a.
```{r}
data(sales)
data(lead)
```


```{r}
ts.plot(sales)
acf(sales)
pacf(sales)
acf2(sales)
```

We can see a cutt of in the PACF, but we have problems with the decreasing of the ACF.
We are going to take first difference. Also there is a lot of trend.
```{r}
ts.plot(diff(sales))
acf2(diff(sales))

```
After the application of the first difference we have an improvement in the trend, and
also it appears to be stationary. 
We can see that both are taling off after the lag 1 both tail off. There aren't not cutoff.
So we fit and SARIMA(1,1,1)

```{r}
model_SARIMA_111_3_35 <- sarima(sales,1,1,1)
sarima(sales,0,1,1)
sarima(sales,1,1,0)

```

We can see that the model fits very well, and the others fit very bad so we select the SARIMA(1,1,1).


b.
```{r}
#We do the cff
ccf2(diff(lead),diff(sales))
lag2.plot(diff(lead),diff(sales),max.lag = 3)
```

We can see that both indicate that a regresion between $\nabla$sales, and $\nabla^{3} lead$ 
could be a good fit.
```{r}
#We do the intersection
sales_lead <- ts.intersect(sales, leadL3 = stats::lag(lead,-3),dframe = TRUE)
summary(regression_3_35 <- lm(sales ~ leadL3, data = sales_lead,na.action = NULL))

```
We can see that all the variables are significant. 

```{r}
#We plot the residuals
ts.plot(resid(regression_3_35))

```

It doesn't look like a white noise so we are going to fit an ARMA to the variable
```{r}
acf2(resid(regression_3_35))

```

There is a tail off in both so we select and ARMA(1,0) because we notice that there is a 
tail off in the ACF, and a cut off after lag 1.

```{r}

ds <- sales_lead[,1]
dl3 <- sales_lead[,2]
fit_final_3_35 <- model_sarima_regression_11_3_35 <- sarima(ds,1,0,0, xreg = dl3)
plot(resid(fit_final_3_35$fit))
acf2(resid(fit_final_3_35$fit))
```


Exercise 3.42
```{r}
data(UnempRate)
ts.plot(UnempRate)
```

It doesn't look stationary, we are going to take first difference.

```{r}
diff_UnempRate <- diff(UnempRate)
ts.plot(diff_UnempRate)
```

It looks like stationary and we have controlled the trend.

```{r}
acf2(diff_UnempRate)
```

We can see that cut off every 12 months in the ACF and in the PACF tails off